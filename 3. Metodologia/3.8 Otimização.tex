\section{Otimização de hiperparâmetros}

A seleção adequada de hiperparâmetros é essencial para garantir a capacidade de generalização dos modelos de previsão. Esse processo poderá ser formulado como o problema de otimização apresentado na Equação (\ref{eq:hyperopt}), no qual se busca a configuração que minimiza a função de perda avaliada sobre o conjunto de validação.

\begin{equation}
\theta^{*} = \arg\min_{\theta \in \Theta} \; \mathcal{L}(M(\theta); \mathcal{D}_{\text{val}})
\label{eq:hyperopt}
\end{equation}

\noindent
Em que $\theta$ representará o vetor de hiperparâmetros; $\Theta$ será o espaço de busca; $M(\theta)$ será o modelo parametrizado; $\mathcal{L}(\cdot)$ corresponderá à função de perda utilizada; e $\theta^{*}$ será a configuração ótima encontrada.

Nesta dissertação será adotada exclusivamente a \textbf{otimização Bayesiana}, devido à sua eficiência na exploração de espaços de busca contínuos e potencialmente de alta dimensionalidade. Diferentemente de métodos exaustivos como \textit{Grid Search}, a otimização Bayesiana evita avaliações redundantes em regiões sabidamente inferiores e concentra recursos computacionais em áreas promissoras do espaço de hiperparâmetros. Essa característica é especialmente relevante para modelos como LSTM e XGBoost, cujos hiperparâmetros interagem de forma não linear e apresentam superfícies de perda complexas.

A abordagem bayesiana modelará a função de perda $\mathcal{L}(\theta)$ como uma função aleatória, atualizada iterativamente após cada avaliação. Seja $\mathcal{M}_t$ o modelo substituto após $t$ iterações (por exemplo, um processo Gaussiano). A escolha do próximo ponto será guiada por uma \textit{função de aquisição} $a(\theta; \mathcal{M}_t)$, conforme a Equação (\ref{eq:bayesopt}), que balanceará exploração de regiões pouco testadas e exploração de regiões potencialmente ótimas.

\begin{equation}
\theta_{t+1} = \arg\max_{\theta \in \Theta} \; a(\theta; \mathcal{M}_t)
\label{eq:bayesopt}
\end{equation}

\noindent
Funções de aquisição clássicas incluem \textit{Expected Improvement}, \textit{Probability of Improvement} e \textit{Upper Confidence Bound}, todas capazes de direcionar a busca para regiões com maior probabilidade de redução da perda. O processo repete-se até atingir convergência, limite de iterações ou outro critério de parada definido.

A escolha pela otimização Bayesiana assegurará maior eficiência computacional, melhor cobertura do espaço de busca e menor risco de convergência para regiões subótimas, permitindo ajustar hiperparâmetros de forma robusta e consistente em todos os modelos avaliados nesta dissertação.
