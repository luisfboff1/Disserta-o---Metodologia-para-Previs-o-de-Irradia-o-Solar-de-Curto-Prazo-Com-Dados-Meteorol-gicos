\section{LSTM}

A implementação dos modelos em \texttt{PyTorch} será organizada em três blocos principais: (i) estrutura de dados e tensores, (ii) modelo LSTM básico e (iii) função orquestradora \texttt{train\_lstm\_forecast}, a qual permitirá treinar não apenas a LSTM, mas também variantes como arquiteturas com atenção.

\subsection{Estrutura de dados e tensores}

Será criada a classe \texttt{TimeSeriesDataset}, herdando de \texttt{torch.utils.data.Dataset}, a qual organizará os dados supervisionados em pares de entrada e saída. A classe converterá as matrizes de janelas $(X,y)$ para tensores \texttt{float32}, garantindo consistência no treinamento. 

Cada lote do \texttt{DataLoader} será estruturado conforme a Equação (\ref{eq:lote_dataloader}).
\begin{equation}
\mathbf{X}_{\mathrm{batch}} \in \mathbb{R}^{B \times L \times p}, \quad \mathbf{y}_{\mathrm{batch}} \in \mathbb{R}^{B \times H}
\label{eq:lote_dataloader}
\end{equation}

\noindent
Em que $B$ será o tamanho do \textit{batch}; $L$ será o tamanho da janela de entrada (\textit{input window}); $p$ será o número de variáveis preditoras (\textit{features}); e $H$ será o horizonte de previsão (\textit{output window}). Essa organização materializará explicitamente o triplo \{features, input window, output window\}, fundamental para a formulação supervisionada.

\subsection{Modelo LSTM}

O modelo \texttt{LSTMForecast} será implementado como uma subclasse de \texttt{torch.nn.Module}, recebendo explicitamente os hiperparâmetros de arquitetura e função de ativação. Sua construção compreenderá duas camadas principais: (i) a rede recorrente LSTM e (ii) uma camada totalmente conectada para mapear o estado oculto para as previsões.

Na inicialização, a classe receberá os argumentos: \texttt{input\_size} (número de \textit{features} em cada passo temporal), \texttt{hidden\_size} (dimensão do estado oculto), \texttt{num\_layers} (número de camadas empilhadas), \texttt{output\_size} (número de passos futuros a prever), \texttt{dropout} (taxa de desativação) e \texttt{activation} (função de ativação final). 

A rede recorrente será instanciada conforme Equação (\ref{eq:lstm_instancia}).
\begin{equation}
\mathrm{LSTM}(p, h, L_r, \texttt{batch\_first=True}, d)
\label{eq:lstm_instancia}
\end{equation}

\noindent
Em que $p$ será o número de \textit{features} (\texttt{input\_size}); $h$ será o tamanho do estado oculto (\texttt{hidden\_size}); $L_r$ será o número de camadas empilhadas (\texttt{num\_layers}); e $d$ será a taxa de dropout aplicada entre camadas.

Após a camada recorrente, será adicionada uma camada linear $\mathbf{W}\in\mathbb{R}^{H\times h}$, $\mathbf{b}\in\mathbb{R}^{H}$, responsável por projetar o estado oculto final em $H$ passos de previsão. Opcionalmente, aplicar-se-á uma função de ativação definida pelo usuário:
\[
\phi \in \{\mathrm{ReLU}, \tanh, \sigma, \mathrm{Identity}\}.
\]

A passagem direta do modelo será descrita pela Equação (\ref{eq:lstm_forward_full}):
\begin{equation}
\begin{aligned}
\mathbf{H}, (\mathbf{h}_t,\mathbf{c}_t) &= \mathrm{LSTM}(\mathbf{X}) \\
\mathbf{z} &= \mathbf{H}_{[:, -1, :]} \\
\widehat{\mathbf{y}} &= \phi(\mathbf{W}\mathbf{z} + \mathbf{b}) \in \mathbb{R}^{H}
\end{aligned}
\label{eq:lstm_forward_full}
\end{equation}

\noindent
Em que $\mathbf{X} \in \mathbb{R}^{B \times L \times p}$ será o lote de entradas; $\mathbf{H} \in \mathbb{R}^{B \times L \times h}$ será a sequência de estados ocultos retornados pela LSTM; $\mathbf{H}_{[:, -1, :]}$ selecionará o último estado oculto da sequência; $\mathbf{z} \in \mathbb{R}^{B \times h}$ será o vetor oculto condensado; e $\widehat{\mathbf{y}} \in \mathbb{R}^{B \times H}$ será o vetor final de previsões, já no domínio escalonado.

A implementação seguirá a estratégia \textit{multi-output direto}, em que todo o horizonte de previsão será produzido de uma única vez a partir do último estado oculto. Isso evitará a acumulação de erro típica de abordagens recursivas e garantirá consistência no alinhamento temporal das saídas.

O uso da função de ativação final $\phi(\cdot)$ será configurável: \texttt{ReLU}, $\tanh$ e $\sigma$ poderão impor restrições de faixa aos valores previstos; enquanto \texttt{Identity} manterá a saída linear, adequada quando as previsões forem revertidas da escala \textit{min--max} para o domínio físico (irradiância em W/m²).

\subsection{Função orquestradora \texttt{train\_lstm\_forecast}}

Será desenvolvida a função \texttt{train\_lstm\_forecast}, responsável por encapsular todo o pipeline de treinamento e avaliação. Essa função realizará:

\begin{enumerate}
    \item Fixação da semente aleatória (\texttt{NumPy}, \texttt{PyTorch}) para reprodutibilidade.
    \item Criação das janelas supervisionadas $(X,y)$ e normalização \textit{min--max}, com ajuste feito apenas sobre o conjunto de treino.
    \item Separação temporal em treino, validação e teste com base em anos específicos.
    \item Criação de \texttt{DataLoader}s otimizados para GPU (\texttt{pin\_memory}, \texttt{non\_blocking}).
    \item Definição da função de perda (\texttt{MSELoss}), otimizador configurável (Adam, SGD, RMSProp, AdamW, etc.) e \textit{weight decay} para regularização.
    \item Treinamento com \textit{mixed precision} (AMP) e critério de \textit{early stopping} baseado na perda de validação.
    \item Avaliação no conjunto de teste, inversão da escala para domínio físico (W/m²) e cálculo de métricas de desempenho.
    \item Espalhamento das previsões $\widehat{\mathbf{y}}$ no índice temporal original, assegurando consistência no horizonte de 15 minutos.
    \item Registro completo do experimento no \texttt{MLflow} (parâmetros, métricas, modelo e artefatos).
\end{enumerate}


\subsection{Parâmetros controlados}

Os principais parâmetros aceitos pela função serão apresentados na Tabela \ref{tab:parametros_treino}.

\begin{table}[H]
    \centering
    \caption{Principais parâmetros da função \texttt{train\_lstm\_forecast}.}
    \resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{|l|p{10cm}|}
        \hline
        \textbf{Parâmetro} & \textbf{Descrição} \\
        \hline
        \texttt{input\_window} ($L$) & Comprimento da janela de entrada (nº de passos históricos). \\
        \hline
        \texttt{output\_window} ($H$) & Horizonte de previsão (nº de passos futuros, aqui 15 min $\times$ 64 para 1 dia). \\
        \hline
        \texttt{features} & Lista de variáveis preditoras utilizadas. \\
        \hline
        \texttt{target} & Variável alvo (irradiância global). \\
        \hline
        \texttt{n\_layers} & Número de camadas LSTM empilhadas. \\
        \hline
        \texttt{hidden\_size} & Dimensão do estado oculto em cada célula LSTM. \\
        \hline
        \texttt{dropout} & Taxa de desativação de neurônios para regularização. \\
        \hline
        \texttt{batch\_size} & Tamanho do mini-batch no treinamento. \\
        \hline
        \texttt{learning\_rate} & Taxa de aprendizado do otimizador. \\
        \hline
        \texttt{optimizer\_name} & Algoritmo de otimização escolhido (Adam, SGD, RMSProp, etc.). \\
        \hline
        \texttt{early\_stopping\_patience} & Nº de épocas sem melhoria para ativar parada antecipada. \\
        \hline
        \texttt{activation\_function} & Função de ativação na camada final (ReLU, Tanh, Sigmoid, Linear). \\
        \hline
        \texttt{weight\_decay} & Penalização L2 nos pesos para regularização. \\
        \hline
        \texttt{mlflow\_experiment\_name} & Nome do experimento no \texttt{MLflow}. \\
        \hline
    \end{tabular}
    }
    \par\small{Fonte: Autor (2025)}
    \label{tab:parametros_treino}
\end{table}



\subsection{Métricas de avaliação}

Após o treinamento, as previsões serão avaliadas em domínio físico (W/m²) por meio de RMSE e $R^2$, definidos nas Equações (\ref{eq:rmse}) e (\ref{eq:r2}). Além do cálculo global, as métricas também serão computadas por horizonte e em janelas diurnas (7h--19h), de modo a focar no período de maior relevância energética.
